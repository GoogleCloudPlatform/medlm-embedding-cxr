{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!--\n",
        "\n",
        "Copyright 2024 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "-->"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JIpyR7i-XtMy"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/medlm-embedding-cxr/raw/main/notebook.ipynb\">\n",
        "      <img src=\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRQBAwQEBQQFCQUFCRQNCw0UFBQUFBQUFBQUFBQUFBQUFBQUFBQTEBQUFBQUFBAUFBQUFBQUFBQVDw8SExISDxQSDv/AABEIACAAIAMBEQACEQEDEQH/xAAZAAEAAwEBAAAAAAAAAAAAAAAGBAUHCQP/xAAsEAABBAEDAgUCBwAAAAAAAAACAQMEBREGEiEAMQcTFCIjQXEVFlFhkaHR/8QAGAEAAwEBAAAAAAAAAAAAAAAAAwQFAgb/xAApEQABAwMDBAEEAwAAAAAAAAABAgMRAAQhEjFBE1FhcSIFMpGhFIGx/9oADAMBAAIRAxEAPwDqTazVrauZLQPMWOybuzON20VXH9dFaR1XEomJIFYcVoQVdq8Km6Yso8PJg3LkRQlem35IQJE5+2VxnojzCmlKx8QSJ9VhtwLA7kTFV+o9aV1GNpFGwrfxuHWO2qQJkwWPgDKea4uFUGtybVc2qic98Y6YtrF1/QsoV01LCNQTOTwNpVGQmc0F66ba1J1DWElUExgcneBO5jFT9N2pXunauyL026ZFakL6N/z2MmCF8bmE3jzwWEymFwmel7loMPrZE/EkZEHBjIzB7icbUZlzqtJcxkA4MjPY4kdjzUi1JQq5hCRCSMmqELiNqntXsS8D917dDay4me48/rmtr+w0UmWTqC9HYenet/L5Ph6cmHD3dkIXCTaTmeyl7F7r1WQ0nC1AaepGdQ/IGQPXy4FIKWYKUkzonEfonE+8UBvm7y3v7Cvs5gP6aleHToyq+VMhsTjlEe03DIc7EUFUVcT4ULroLc2zLKHWUw6m5EKCVlGkDAAO+chJ+ZFRnEXDzqmnlS0pmCklIVqO5JG2MEj4ztWpeHUFis8PtMQ4rflRo9XFaaBZAyNoC0KInmh7XOET3DwvdOF65X6k4p29fcWZJWonBGSTwcj0cjY1fs20tWzTaBACQBmcAd+ffNWF/GnSq0wr3GQfznZIDe24P1Av2X9el7dTaHAXQY8bjzRnQtSYRv5oXaPM6iqL2tksu100qd+CdWzGAnlRQL3MlxvTHCBlEz/PVppKrZ1p5B1J1hWokxuPu3jyd/8AKnOaX0LbUIOkiIE/139UUkWUTSLtBWwWHNR3LmmmahvSzlewL+3Aqrkp3lWgx7SbUlDnOF7pXS0u8Drzh6aA6V9TUqOcITgKM5ConjxSJWm26bSBrVoCdECeMqPA4ImK0/QdZc1Om48e8ehHNHszXMeVHjN8ILLafURRMIv+dctfu27z5XbA6e6jJJ5J8mrVqh1toJeInwIA8D1SHqdTdHNdaPDWVIUUXkiTAcbdYlbVXaomJKBYUVUDQdhIhIu0lwqLhUpWF4bJ3WRKcgj2CJ5EjcSDkZBGKUubf+QjSDBEEH0Z/B2PjaDmovh14eQvD+rfbaRp+xlvuSJc0W1EnCNwjQEypFsBC2iikuERMqqqqqX6l9Rc+oOAnCUgADtAAngSYkmBnsIAxaWiLVJjKiSSfZn8DYeKW9SKer//2Q==\" alt=\"Vertex AI logo\">\n",
        "      Run in Vertex AI Workbench\n",
        "    </a>\n",
        "    </td>\n",
        "    <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/GoogleCloudPlatform/medlm-embedding-cxr/blob/main/notebook.ipynb\"><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgBAMAAACBVGfHAAAALVBMVEVHcEz0mQP6rgD6rAD6rQD0mwPxjwX5qwD6qwDocArzlQP9uQDobwrobwrocAqx6FUsAAAAD3RSTlMAL4e95XEUn//k//+R/8OlcE7aAAAA0klEQVR4AcXQPwhBURQG8A/Jgt4+SCmzZDAKySiD2UBZlAkzyimLSVH2JGaZ3l5WRe97u8I+Ojf1kJ3f8nW7t3v+4Fd8iWKhBjQzxbQFoy8io3iwrDGGaoiR64tRArDYiWqKMVkBYa5HveygJZKvlve00eY5CV83KwPAv+ERBzqmkkhHwxxunELvRKAidLGkDSAkQ6goTyArAALPFsLk9wvvj7pGm+6rSkrjRsfrY7K1ECOPXqd7uneStjfLhsbpNe2BxuxtH0uSzsfGLvOrhX95AI3Oednh8+dnAAAAAElFTkSuQmCC\" alt=\"Colab logo\">\n",
        "    Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/GoogleCloudPlatform/medlm-embedding-cxr/\"><img src=\"data:image/png;base64,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\" alt=\"GitHub logo\">\n",
        "    View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Google Cloud MedLM Embedding API For CXR\n",
        "\n",
        "Once you have labeled datasets we will ingest them in Healthcare API, use BigQuery to filter the cohorts of information, retrieve the relevant labeled images, train using the CXR foundation model, and ultimately test our trained model on staged data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prerequisites\n",
        "\n",
        "- Make sure you have been granted access to the MedLM Embedding API For CXR by filling out [this](https://docs.google.com/forms/d/e/1FAIpQLSd-fBqL-Ox5Qqmr6Q7nRo2oTfbttgdr700-XjSV4GEKYhbicg/viewform) form.\n",
        "- After you've been given access to the API, you only have to change the `PROJECT_ID` value below to your own project, and then you can Run All cells.\n",
        "- Code was tested with Python v3.10.6 in VSCode, Vertex AI Workbench, and Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random, sys, time, os\n",
        "\n",
        "# You must update the variables in this section\n",
        "PROJECT_ID = \"[CHANGEME]\"  # @param {type: 'string'}\n",
        "\n",
        "# Consts for your GCP environment\n",
        "RAND = random.randint(1,100000)\n",
        "LOCATION = \"us-central1\" # This region required for early access.\n",
        "DATASET_ID = f\"dataset_{RAND}\"\n",
        "STORE_ID = \"store_id\"\n",
        "HCAPI_HOST = f\"https://healthcare.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/datasets/{DATASET_ID}/dicomStores/{STORE_ID}\"\n",
        "DICOMWEB_HOST = f\"{HCAPI_HOST}/dicomWeb\"\n",
        "BQ_TABLE_ID = \"metadata\"\n",
        "BQ_TABLE = f\"{PROJECT_ID}.{DATASET_ID}.{BQ_TABLE_ID}\"\n",
        "VERTEX_ENDPOINT_ID = f\"cxr-endpoint-{RAND}\"\n",
        "MODEL_BUCKET_NAME = f\"cxr_model_bucket_{RAND}\"\n",
        "\n",
        "# Other Consts\n",
        "DIAGNOSIS = \"PNEUMOTHORAX\"\n",
        "STAGED_DIR = \"./data/staged/\"\n",
        "EMBEDDINGS_DIR = \"./data/outputs\"\n",
        "MODEL_DIR = \"./data/outputs/model\"\n",
        "MIN_STUDIES = 195\n",
        "EMBEDDING_KEY = 'embedding'\n",
        "EMBEDDINGS_SIZE = 32 * 768 # dimensional vector\n",
        "\n",
        "# Authenticate; will only run if you're in Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    # Authenticate user for access. There will be a popup asking you to sign in with your user and approve access.\n",
        "    auth.authenticate_user()\n",
        "\n",
        "# Some basic input validation\n",
        "if PROJECT_ID == \"[CHANGEME]\":\n",
        "    raise ValueError(\"Please provide your own value for PROJECT_ID\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment setup\n",
        "\n",
        "\n",
        "In addition to installing some local python packages, this will use Terraform to set up a [Healthcare API DICOM store](https://cloud.google.com/healthcare-api/docs/how-tos/dicom#creating_a_dicom_store), [BigQuery table](https://cloud.google.com/bigquery/docs/tables), and [streaming](https://cloud.google.com/healthcare-api/docs/how-tos/dicom-bigquery-streaming) of metadata from Healthcare API DICOM store to a BigQuery table. Additionally it will setup a Google Cloud Storage bucket to copy your model and it will create a Vertex AI Online Prediction endpoint to host the model you will eventually build. \n",
        "\n",
        "**NOTE:** Please enable all the appropriate APIs based on the above list. Additionally, you must grant the bigquery.dataEditor role to your project's Cloud Healthcare Service Agent, in order to have metadata streamed from Healthcare API DICOM store to BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash -s \"$LOCATION\" \"$PROJECT_ID\" \"$DATASET_ID\" \"$STORE_ID\" \"$BQ_TABLE_ID\" \"$VERTEX_ENDPOINT_ID\" \"$MODEL_BUCKET_NAME\" \"$STAGED_DIR\"\n",
        "\n",
        "# Python packages\n",
        "pip install db-dtypes scikit-learn \"tf-models-official>=2.13.0\"\n",
        "\n",
        "# Package for CXR foundation model wrapper\n",
        "CXR_COMMIT='6c284364e3883353ed737545e475a1016620fd7e' # tested version locked\n",
        "CXR_URL=https://github.com/Google-Health/imaging-research/archive/${CXR_COMMIT}.zip\n",
        "if ! test -d imaging-research-${CXR_COMMIT}; then\n",
        "    curl -L -o imaging-research-${CXR_COMMIT}.zip ${CXR_URL}\n",
        "    unzip -o imaging-research-${CXR_COMMIT}.zip\n",
        "    pip install imaging-research-${CXR_COMMIT}/cxr-foundation/\n",
        "    rm -rf imaging-research-${CXR_COMMIT}.zip\n",
        "fi\n",
        "\n",
        "# Retrieve staged data\n",
        "if ! test -d data; then\n",
        "    mkdir -p data/staged\n",
        "    gsutil -m cp -r gs://mis-ai-accelerator/data/staged/*.dcm data/staged/\n",
        "fi\n",
        "\n",
        "# Retrieve terraform executable and scripts locally\n",
        "TF_URL=https://releases.hashicorp.com/terraform/1.6.1/terraform_1.6.1_linux_amd64.zip\n",
        "if ! test -f terraform; then\n",
        "    curl -so terraform.zip ${TF_URL}\n",
        "    unzip terraform.zip\n",
        "    rm -rf terraform.zip\n",
        "    gsutil -m cp -r gs://mis-ai-accelerator/tf .\n",
        "fi\n",
        "\n",
        "# Deploy terraform\n",
        "./terraform -chdir=./tf init\n",
        "./terraform -chdir=./tf plan -var=\"project_id=$2\" -var=\"location=$1\" -var=\"dataset_id=$3\" -var=\"store_id=$4\" -var=\"table_id=$5\" -var=\"vertex_endpoint_name=$6\" -var=\"gcs_bucket_name=$7\" -out tf.plan\n",
        "./terraform -chdir=./tf apply tf.plan\n",
        "\n",
        "# Label the environment, if it is GCE\n",
        "STATUS_CODE=$(curl --write-out %{http_code} --silent --output /dev/null metadata)\n",
        "if [[ \"${STATUS_CODE}\" -eq 200 ]]; then\n",
        "    VMNAME=$(curl -H Metadata-Flavor:Google metadata/computeMetadata/v1/instance/hostname | cut -d. -f1)\n",
        "    ZONE=$(curl -H Metadata-Flavor:Google metadata/computeMetadata/v1/instance/zone | cut -d/ -f4)\n",
        "    gcloud compute instances update ${VMNAME} --zone=${ZONE} --update-labels=goog-packaged-solution=medical-imaging-suite\n",
        "    echo \"Set label on ${VMNAME}\"\n",
        "else\n",
        "    echo \"Skipping label since not inside a GCE instance.\"\n",
        "fi\n",
        "\n",
        "# Import into Healthcare API, so embeddings API can access DICOM images\n",
        "gcloud healthcare dicom-stores import gcs $4 --dataset=$3 --project=$2 --location=$1 --gcs-uri=\"gs://mis-ai-accelerator/data/staged/inputs/*.dcm\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Map labels from DICOM using BigQuery\n",
        "\n",
        "Execute a query against BQ to get the list of DICOM instances that are positive and negative for Pneumothorax, based on presence of KOS.\n",
        "\n",
        "**NOTE:** If you've just recently pushed a bunch of images to the Healthcare API DICOM Store, then it might take a few seconds to propagate the metadata to BigQuery. You can configure how long this section retries for.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Initialize our BigQuery client (need project ID for Colab)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "#   DICOM types\n",
        "#   Key Object Selection Document, SOPClassUID=1.2.840.10008.5.1.4.1.1.88.59, Modality=KO\n",
        "#   Secondary Capture Image Storage, SOPClassUID=1.2.840.10008.5.1.4.1.1.7, Modality=DX\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "  StudyInstanceUID,\n",
        "  SeriesInstanceUID,\n",
        "  SOPInstanceUID,\n",
        "  CAST (CAST (StudyInstanceUID IN (\n",
        "      SELECT\n",
        "        DISTINCT StudyInstanceUID\n",
        "      FROM\n",
        "        `{}`\n",
        "      WHERE\n",
        "        SOPClassUID='1.2.840.10008.5.1.4.1.1.88.59'\n",
        "        AND Modality='KO') AS INT64) AS FLOAT64) AS {}\n",
        "FROM\n",
        "  `{}`\n",
        "WHERE\n",
        "  SOPClassUID='1.2.840.10008.5.1.4.1.1.7'\n",
        "  AND Modality='DX'\n",
        "\"\"\".format(\n",
        "    BQ_TABLE, DIAGNOSIS, BQ_TABLE\n",
        ")\n",
        "\n",
        "bq_job_config = bigquery.QueryJobConfig(use_query_cache=False)\n",
        "\n",
        "# Waiting for BQ to be populated with enough metadata\n",
        "totalWait, INCREMENT, MAX_WAIT = 0, 30, 1200  # Max wait = 20 mins!\n",
        "while True:\n",
        "    bq_df = bq_client.query(query, bq_job_config).to_dataframe()\n",
        "    if len(bq_df) >= MIN_STUDIES:\n",
        "        print(f\"Completed populating BQ! ({totalWait} sec)\")\n",
        "        break\n",
        "    print(f\"Progress populating BQ: {len(bq_df)}/{MIN_STUDIES} ({totalWait} sec)\")\n",
        "    totalWait += INCREMENT\n",
        "    if totalWait >= MAX_WAIT:\n",
        "        raise ValueError(\n",
        "            f\"Max time elapsed populating BQ! Please try this cell again. Progress: {len(bq_df)}/{MIN_STUDIES}\"\n",
        "        )\n",
        "    time.sleep(INCREMENT)\n",
        "\n",
        "# Paths for generated embeddings\n",
        "bq_df[\"embedding_file\"] = bq_df[\"SOPInstanceUID\"].apply(\n",
        "    lambda x: os.path.join(EMBEDDINGS_DIR, x + \".tfrecord\")\n",
        ")\n",
        "\n",
        "display(bq_df.tail())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate embeddings for all DICOM images\n",
        "\n",
        "Call the MedLM Embedding API For CXR with a reference to each DICOM image in the Healthcare API DICOM store, generate an embedding, and store it locally for use in training later. This may take ~15 mins, depending on data being sent, your connection latency, and load on server. You may think of embeddings as compressed raster images, in a format efficient for model training.\n",
        "\n",
        "**IMPORTANT:** You must have access to use the MedLM Embedding API for CXR. For access, please fill out [this](https://docs.google.com/forms/d/e/1FAIpQLSd-fBqL-Ox5Qqmr6Q7nRo2oTfbttgdr700-XjSV4GEKYhbicg/viewform) form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from google.auth.transport.requests import AuthorizedSession\n",
        "import google.auth\n",
        "\n",
        "_CXR_URL = f\"https://{LOCATION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/medlm-embedding-cxr:predict\"\n",
        "\n",
        "credentials, _ = google.auth.default()\n",
        "api_session = AuthorizedSession(credentials)\n",
        "\n",
        "\n",
        "def retrieve_embedding_array(dicomUri: str) -> np.ndarray:\n",
        "    json = {\n",
        "        \"instances\": [{\"dicomUri\": dicomUri}],\n",
        "    }\n",
        "\n",
        "    while True:  # Basic retry, no limit for HTTP 429 (due to quota)\n",
        "        response = api_session.post(_CXR_URL, json=json)\n",
        "        if response.status_code == 429:\n",
        "            time.sleep(5)\n",
        "            continue\n",
        "        if not response.ok:\n",
        "            raise RuntimeError(f\"Embedding creation call failed: {response.text}\")\n",
        "        break\n",
        "\n",
        "    embed = response.json()[\"predictions\"][0][\"chestXRayEmbedding\"]\n",
        "    return np.array(embed, dtype=np.float32)\n",
        "\n",
        "\n",
        "if not os.path.exists(EMBEDDINGS_DIR):\n",
        "    os.makedirs(EMBEDDINGS_DIR)\n",
        "\n",
        "i, total = 0, len(bq_df)\n",
        "for _, row in bq_df.iterrows():\n",
        "    i += 1\n",
        "    if os.path.exists(row[\"embedding_file\"]):\n",
        "        print(f'[{i}/{total}] Embedding file exists; skipping: {row[\"embedding_file\"]}')\n",
        "        continue\n",
        "    dicomUri = f'{DICOMWEB_HOST}/studies/{row[\"StudyInstanceUID\"]}/series/{row[\"SeriesInstanceUID\"]}/instances/{row[\"SOPInstanceUID\"]}'\n",
        "    arr = retrieve_embedding_array(dicomUri=dicomUri)\n",
        "    example = tf.train.Example()\n",
        "    example.features.feature[EMBEDDING_KEY].float_list.value[:] = arr.flatten()\n",
        "    with tf.io.TFRecordWriter(row[\"embedding_file\"]) as w:\n",
        "        w.write(example.SerializeToString())\n",
        "\n",
        "    print(f'[{i}/{total}] Generated embedding file: {row[\"embedding_file\"]}')\n",
        "\n",
        "print(\"Completed embedding file creation\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fVGYhxEkWBhs"
      },
      "source": [
        "## Train A Model\n",
        "\n",
        "Finally, we can train a model using the embeddings! With a simple feed-forward neural network, it should take < 5 minutes to train 100 epochs! No GPU required.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow_models as tfm\n",
        "from typing import Iterable\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def parse_serialized_example_values(\n",
        "    serialized_example: bytes,\n",
        ") -> tf.Tensor:\n",
        "    features = {\n",
        "        EMBEDDING_KEY: tf.io.FixedLenFeature(\n",
        "            [EMBEDDINGS_SIZE],\n",
        "            tf.float32,\n",
        "            default_value=tf.constant(0.0, shape=[EMBEDDINGS_SIZE]),\n",
        "        )\n",
        "    }\n",
        "    parsed_tensors = tf.io.parse_example(serialized_example, features=features)\n",
        "    return parsed_tensors[EMBEDDING_KEY]\n",
        "\n",
        "\n",
        "def get_dataset(filenames: Iterable[str], labels: Iterable[int]) -> tf.data.Dataset:\n",
        "    ds_embeddings = tf.data.TFRecordDataset(\n",
        "        filenames, num_parallel_reads=tf.data.AUTOTUNE\n",
        "    ).map(parse_serialized_example_values)\n",
        "    ds_labels = tf.data.Dataset.from_tensor_slices(labels)\n",
        "\n",
        "    return tf.data.Dataset.zip((ds_embeddings, ds_labels))\n",
        "\n",
        "\n",
        "def create_model(\n",
        "    heads,\n",
        "    learning_rate=0.1,\n",
        "    end_lr_factor=1.0,\n",
        "    dropout=0.0,\n",
        "    decay_steps=1000,\n",
        "    loss_weights=None,\n",
        "    hidden_layer_sizes=[512, 256],\n",
        "    weight_decay=0.0,\n",
        "    seed=None,\n",
        ") -> tf.keras.Model:\n",
        "    # Creates linear probe or multilayer perceptron using LARS + cosine decay.\n",
        "    inputs = tf.keras.Input(shape=(32 * 768,))  # Based on ELIXR\n",
        "    inputs_reshape = tf.keras.layers.Reshape((32, 768))(inputs)\n",
        "    inputs_pooled = tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_last\")(\n",
        "        inputs_reshape\n",
        "    )\n",
        "    hidden = inputs_pooled\n",
        "    # If no hidden_layer_sizes are provided, model will be a linear probe.\n",
        "    for size in hidden_layer_sizes:\n",
        "        hidden = tf.keras.layers.Dense(\n",
        "            size,\n",
        "            activation=\"relu\",\n",
        "            kernel_initializer=tf.keras.initializers.HeUniform(seed=seed),\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(l2=weight_decay),\n",
        "            bias_regularizer=tf.keras.regularizers.l2(l2=weight_decay),\n",
        "        )(hidden)\n",
        "        hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
        "        hidden = tf.keras.layers.Dropout(dropout, seed=seed)(hidden)\n",
        "    output = tf.keras.layers.Dense(\n",
        "        units=len(heads),\n",
        "        activation=\"sigmoid\",\n",
        "        kernel_initializer=tf.keras.initializers.HeUniform(seed=seed),\n",
        "    )(hidden)\n",
        "\n",
        "    outputs = {}\n",
        "    for i, head in enumerate(heads):\n",
        "        outputs[head] = tf.keras.layers.Lambda(\n",
        "            lambda x: x[..., i : i + 1], name=head.lower()\n",
        "        )(output)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    learning_rate_fn = tf.keras.experimental.CosineDecay(\n",
        "        tf.cast(learning_rate, tf.float32),\n",
        "        tf.cast(decay_steps, tf.float32),\n",
        "        alpha=tf.cast(end_lr_factor, tf.float32),\n",
        "    )\n",
        "    model.compile(\n",
        "        optimizer=tfm.optimization.lars.LARS(learning_rate=learning_rate_fn),\n",
        "        loss=dict([(head, \"binary_crossentropy\") for head in heads]),\n",
        "        loss_weights=loss_weights or dict([(head, 1.0) for head in heads]),\n",
        "        weighted_metrics=[\n",
        "            tf.keras.metrics.FalsePositives(),\n",
        "            tf.keras.metrics.FalseNegatives(),\n",
        "            tf.keras.metrics.TruePositives(),\n",
        "            tf.keras.metrics.TrueNegatives(),\n",
        "            tf.keras.metrics.AUC(),\n",
        "            tf.keras.metrics.AUC(curve=\"PR\", name=\"auc_pr\"),\n",
        "        ],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# Create train and validation split (train = 70%, validation = 30%)\n",
        "df_train, df_validate = train_test_split(bq_df, train_size=0.7)\n",
        "\n",
        "# Create training and validation datasets\n",
        "training_data = get_dataset(\n",
        "    filenames=df_train[\"embedding_file\"].values, labels=df_train[DIAGNOSIS].values\n",
        ")\n",
        "\n",
        "\n",
        "validation_data = get_dataset(\n",
        "    filenames=df_validate[\"embedding_file\"].values, labels=df_validate[DIAGNOSIS].values\n",
        ")\n",
        "\n",
        "# Create and train the model\n",
        "model = create_model([DIAGNOSIS])\n",
        "\n",
        "model.fit(\n",
        "    x=training_data.batch(512).prefetch(tf.data.AUTOTUNE).cache(),\n",
        "    validation_data=validation_data.batch(1).cache(),\n",
        "    epochs=100,\n",
        ")\n",
        "\n",
        "# Summary after training is complete\n",
        "model.summary()\n",
        "\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    os.makedirs(MODEL_DIR)\n",
        "\n",
        "# Save the model locally\n",
        "tf.saved_model.save(model, MODEL_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Examine metrics\n",
        "\n",
        "Graph the metrics for the model based on validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rows = []\n",
        "for embeddings, label in validation_data.batch(1):\n",
        "    row = {\n",
        "        f\"{DIAGNOSIS}_prediction\": model(embeddings)[DIAGNOSIS].numpy().flatten()[0],\n",
        "        f\"{DIAGNOSIS}_value\": label.numpy().flatten()[0],\n",
        "    }\n",
        "    rows.append(row)\n",
        "eval_df = pd.DataFrame(rows)\n",
        "eval_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import sklearn\n",
        "\n",
        "\n",
        "def plot_curve(x, y, auc, x_label=None, y_label=None, label=None):\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  plt.plot(x, y, label=f'{label} (AUC: %.3f)' % auc, color='black')\n",
        "  plt.legend(loc='lower right', fontsize=18)\n",
        "  plt.xlim([-0.01, 1.01])\n",
        "  plt.ylim([-0.01, 1.01])\n",
        "  if x_label:\n",
        "    plt.xlabel(x_label, fontsize=24)\n",
        "  if y_label:\n",
        "    plt.ylabel(y_label, fontsize=24)\n",
        "  plt.xticks(fontsize=12)\n",
        "  plt.yticks(fontsize=12)\n",
        "  plt.grid(visible=True)\n",
        "\n",
        "labels = eval_df[f'{DIAGNOSIS}_value'].values\n",
        "predictions = eval_df[f'{DIAGNOSIS}_prediction'].values\n",
        "false_positive_rate, true_positive_rate, thresholds = sklearn.metrics.roc_curve(\n",
        "    labels,\n",
        "    predictions,\n",
        "    drop_intermediate=False)\n",
        "auc = sklearn.metrics.roc_auc_score(labels, predictions)\n",
        "plot_curve(false_positive_rate, true_positive_rate, auc, x_label='False Positive Rate', y_label='True Positive Rate', label=DIAGNOSIS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy the model and test it\n",
        "The following section shows how to deploy the exported model to GCP and then test inference by creating the embeddings API and then online prediction endpoint where your model is hosted.\n",
        "\n",
        "**NOTE**: The deployment of the model to the endpoint could take ~15 mins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash -s \"$LOCATION\" \"$MODEL_DIR\" \"$MODEL_BUCKET_NAME\" \"$VERTEX_ENDPOINT_ID\" \"$PROJECT_ID\"\n",
        "\n",
        "MODEL_NAME=cxr-model\n",
        "\n",
        "echo Copying model to GCS bucket\n",
        "gsutil cp -r $2 gs://$3\n",
        "\n",
        "echo Uploading the model to Vertex AI Model Registry \n",
        "gcloud ai models upload \\\n",
        "  --project=$5 \\\n",
        "  --region=$1 \\\n",
        "  --display-name=$MODEL_NAME \\\n",
        "  --container-image-uri=us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-13:latest \\\n",
        "  --artifact-uri=gs://$3/model\n",
        "MODEL_ID=$(gcloud ai models list --project=$5 --region=$1 | grep $MODEL_NAME | awk '{print $1}' | head -1)\n",
        "echo ModelID: $MODEL_ID\n",
        "\n",
        "echo Hosting the model on a Vertex AI endpoint\n",
        "gcloud ai endpoints deploy-model $4 \\\n",
        "  --project=$5 \\\n",
        "  --region=$1 \\\n",
        "  --model=$MODEL_ID \\\n",
        "  --display-name=$MODEL_NAME \\\n",
        "  --traffic-split=0=100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run predictions on images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import pydicom\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_MY_MODEL_URL = f\"https://{LOCATION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{VERTEX_ENDPOINT_ID}:predict\"\n",
        "\n",
        "\n",
        "def get_online_prediction(instance):\n",
        "    # Call CXR to generate embeddings\n",
        "    input = retrieve_embedding_array(dicomUri=instance)\n",
        "    json = { \"instances\": input.reshape(1, EMBEDDINGS_SIZE).tolist() }\n",
        "    # Call my model with the embedding and get prediction\n",
        "    response = api_session.post(_MY_MODEL_URL, json=json)\n",
        "    if not response.ok:\n",
        "        raise RuntimeError(f\"Prediction call failed: {response.content}\")\n",
        "    res = response.json()\n",
        "    return res\n",
        "\n",
        "# Postive prediction\n",
        "pos_file = STAGED_DIR + \"positive.dcm\"\n",
        "pos_instance = f'{DICOMWEB_HOST}/studies/1.3.6.1.4.1.11129.5.5.125244073909057181345738889085284198099651/series/1.3.6.1.4.1.11129.5.5.138502690012449501224934835894513332244191/instances/1.3.6.1.4.1.11129.5.5.144821354400097386866710469118057421849850'\n",
        "pos_predict = get_online_prediction(pos_instance)\n",
        "\n",
        "# Negative prediction\n",
        "neg_file = STAGED_DIR + \"negative.dcm\"\n",
        "neg_instance = f'{DICOMWEB_HOST}/studies/1.3.6.1.4.1.11129.5.5.195421166273048982066192993508222383663074/series/1.3.6.1.4.1.11129.5.5.131098996341768723770121426857691412090508/instances/1.3.6.1.4.1.11129.5.5.164778748062349548634537702002731919155437'\n",
        "neg_predict = get_online_prediction(neg_instance)\n",
        "\n",
        "# Display images\n",
        "_, axes = plt.subplots(1, 2, figsize=(15, 15))\n",
        "axes[0].imshow(pydicom.dcmread(pos_file).pixel_array, cmap='gray')\n",
        "axes[0].set_title(f\"\\\"Positive\\\" {DIAGNOSIS.lower()} prediction: {pos_predict['predictions'][0][0]:.3f}\")\n",
        "axes[0].set_axis_off()\n",
        "axes[1].imshow(pydicom.dcmread(neg_file).pixel_array, cmap='gray')\n",
        "axes[1].set_title(f\"\\\"Negative\\\" {DIAGNOSIS.lower()} prediction: {neg_predict['predictions'][0][0]:.3f}\")\n",
        "axes[1].set_axis_off();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup resources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# TODO[JK]: Remove model from endpoint before destroy\n",
        "# Uncomment to destroy resources (default=OFF for Run All)\n",
        "# ./terraform -chdir=./tf destroy -auto-approve"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8LpEO7UrU9eS"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "d3ac608b8f9188be2227ae82298dfd5de684cbdc4496f362d4b3b9040509447c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
